<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Cédric Gerbelot</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Cédric Gerbelot</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Cédric Gerbelot</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo1.jpeg" alt="" width="350px" />&nbsp;</td>
<td align="left"><h2>Briefly</h2>
<p>I am a Courant Instructor at the Courant Institute of Mathematical Sciences, NYU. I obtained my PhD at the end of August 2022 at Ecole Normale Supérieure de Paris under the supervision of <a href="https://florentkrzakala.com/">Florent Krzakala</a> (EPFL) and <a href="https://www.di.ens.fr/~lelarge/">Marc Lelarge</a> (INRIA, ENS). I work at the interface between theoretical machine learning and mathematical physics. More precisely, I am interested in high-dimensional probability, optimization and mathematical methods inspired by spin glass theory.</p>
<p>Here is a <a href="CV_Courant.pdf">CV</a>.</p>
<h2>Contact</h2>
<ul>
<li><p>E-mail: cedric [dot] gerbelot [at] cims [dot] nyu [dot] edu <br /></p>
</li>
<li><p>Physical address: Courant Institute, 251 Mercer Street, Office 925</p> - New York 10012
</li>
</ul>
</td></tr></table>
<h2>Publications and Preprints</h2>
<p>You can also find my publications on <a href="https://scholar.google.com/citations?user=Ct53LHIAAAAJ&amp;hl=fr">Google Scholar</a>, and some code on <a href="https://github.com/cgerbelo">Github</a>.</p>
<ul>
<li><p>Gerbelot, C., Troiani, E., Mignacco, F., Krzakala, F., Zdeborova, L. <b> Rigorous dynamical men field theory for stochastic gradient descent methods</b>, preprint, 2022. <br />[<a href="https://arxiv.org/abs/2210.06591">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b>  We prove closed-form equations for the exact high-dimensional asymptotics of a family of first order gradient-based methods, learning an estimator (e.g. M-estimator, shallow neural network, ...)
from observations on Gaussian data with empirical risk minimization. This includes widely used algorithms such as stochastic gradient descent (SGD) or Nesterov acceleration. The obtained equations match those resulting from the discretization of dynamical mean-field theory (DMFT) equations from statistical physics when 
applied to gradient flow. Our proof method allows us to 
give an explicit description of how memory kernels build up in the effective dynamics, and to include non-separable update functions, allowing datasets with non-identity covariance matrices. Finally, we provide numerical implementations of the equations for SGD with generic extensive batch-size and with constant learning rates. 
</div></p>
</li>
</ul>   
<ul>
<li><p>Daniels, M., Gerbelot, C., Krzakala, F., Zdeborova, L. <b> Multi-layer State Evolution Under Random Convolutional Design </b>, Advances in Neural Information Processing Systems (Neurips), 2022. <br />[<a href="https://arxiv.org/abs/2205.13503">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b>  Signal recovery under generative neural network priors has emerged as a promising direction in statistical inference and computational imaging. Theoretical analysis of reconstruction algorithms under generative priors is, however, challenging. For generative priors with fully connected layers and Gaussian i.i.d. weights, this was achieved by the multi-layer approximate message (ML-AMP) algorithm via a rigorous state evolution. However, practical generative priors are typically convolutional, allowing for computational benefits and inductive biases, and so the Gaussian i.i.d. weight assumption is very limiting. In this paper, we overcome this limitation and establish the state evolution of ML-AMP for random convolutional layers. We prove in particular that random convolutional layers belong to the same universality class as Gaussian matrices. Our proof technique is of an independent interest as it establishes a mapping between convolutional matrices and spatially coupled sensing matrices used in coding theory. 
</div></p>
</li>
</ul>  
<ul>
<li><p>Loureiro, B., Gerbelot, C., Refinetti, M, Sicuro, G., Krzakala, F. <b>Fluctuations, Bias, Variance and Ensemble of Learners: Exact Asymptotics for Convex Losses in High-Dimension </b>, International Conference on Machine Learning (ICML), 2022. <br />[<a href="https://arxiv.org/abs/2201.13383">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> From the sampling of data to the initialisation of parameters, randomness is ubiquitous in modern Machine Learning practice. Understanding the statistical fluctuations engendered by the different sources of randomness in prediction is therefore key to understanding robust generalisation. In this manuscript we develop a quantitative and rigorous theory for the study of fluctuations in an ensemble of generalised linear models trained on different, but correlated, features in high-dimensions.  In particular, we provide a complete description of the asymptotic joint distribution of the empirical risk minimiser for generic convex loss and regularisation in the high-dimensional limit. Our result encompasses a rich set of classification and regression tasks, such as the lazy regime of overparametrised neural networks, or equivalently the random features approximation of kernels. While allowing to study directly the mitigating effect of ensembling (or bagging) on the bias-variance decomposition of the test error, our analysis also helps disentangle the contribution of statistical fluctuations, and the singular role played by the interpolation threshold
that are at the roots of the ``double-descent'' phenomenon. 
</div></p>
</li>
</ul>  
<ul>
<li><p>Cornacchia, E., Mignacco, F., Veiga, R., Gerbelot, C., Loureiro, B., Zdeborova, L. <b>Learning Curves for the Multi-class Teacher-student Perceptron </b>, 2022, Machine Learning : Science and Technology. <br />[<a href="https://arxiv.org/abs/2203.12094">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b>One of the most classical results in high-dimensional learning theory provides a closed-form expression for the generalisation error of binary classification with the single-layer teacher-student perceptron on i.i.d. Gaussian inputs. Both Bayes-optimal estimation and empirical risk minimisation (ERM) were extensively analysed for this setting. At the same time, a considerable part of modern machine learning practice concerns multi-class classification. Yet, an analogous analysis for the corresponding multi-class teacher-student perceptron was missing. In this manuscript we fill this gap by deriving and evaluating asymptotic expressions for both the Bayes-optimal and ERM generalisation errors in the high-dimensional regime.
</div></p>
</li>
</ul>
<ul>
<li><p>Gerbelot, C. and Berthier, R. <b>Graph-based Approximate Message Passing Iterations</b>, 2021, in review. <br />[<a href="https://arxiv.org/abs/2109.11905">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Approximate-message passing (AMP) algorithms have become an important element of high-dimensional statistical inference, mostly due to their adaptability and concentration properties, the state evolution (SE) equations. This is demonstrated by the growing number of new iterations proposed for increasingly complex problems, ranging from multi-layer inference to low-rank matrix estimation with elaborate priors. In this paper, we address the following questions: is there a structure underlying all AMP iterations that unifies them in a common framework? Can we use such a structure to give a modular proof of state evolution equations, adaptable to new AMP iterations without reproducing each time the full argument ? We propose an answer to both questions, showing that AMP instances can be generically indexed by an oriented graph. This enables to give a unified interpretation of these iterations, independent from the problem they solve, and a way of composing them arbitrarily. We then show that all AMP iterations indexed by such a graph admit rigorous SE equations, extending the reach of previous proofs, and proving a number of recent heuristic derivations of those equations. Our proof naturally includes non-separable functions and we show how existing refinements, such as spatial coupling or matrix-valued variables, can be combined with our framework.
</div></p>
</li>
</ul>
<ul>
<li><p>Loureiro, B., Sicuro, G., Gerbelot, C.,Pacco, A., Krzakala, F, Zdeborova, L. <b>Learning Gaussian Mixtures with Generalized Linear Models : Precise Asymptotics in High-dimensions</b>, Advances in Neural Information Processing Systems (Neurips) 2021, Spotlight presentation. <br />[<a href="https://arxiv.org/abs/2106.03791">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Generalised linear models for multi-class classification problems are one of the fundamental building blocks of modern machine learning tasks. In this manuscript, we characterise the learning of a mixture of <img class="eq" src="eqs/9600028874-130.png" alt="K" style="vertical-align: -0px" /> Gaussians with generic means and covariances via empirical risk minimisation (ERM) with any convex loss and regularisation. In particular, we prove exact asymptotics characterising the ERM estimator in high-dimensions, extending several previous results about Gaussian mixture classification in the literature. We exemplify our result in two tasks of interest in statistical learning: a) classification for a mixture with sparse means, where we study the efficiency of <img class="eq" src="eqs/7513213165401213731-130.png" alt="l_1" style="vertical-align: -3px" /> penalty with respect to <img class="eq" src="eqs/7513213165401213728-130.png" alt="l_2" style="vertical-align: -3px" />; b) max-margin multi-class classification, where we characterise the phase transition on the existence of the multi-class logistic maximum likelihood estimator for <img class="eq" src="eqs/7779556930681419564-130.png" alt="K&gt;2" style="vertical-align: -1px" />. Finally, we discuss how our theory can be applied beyond the scope of synthetic data, showing that in different cases Gaussian mixtures capture closely the learning curve of classification tasks in real data sets.
</div></p>
</li>
</ul>
<ul>
<li><p>Loureiro, B., Gerbelot, C, Cui, H, Goldt, S, Mezard, M, Krzakala, F, Zdeborova, L. <b>Capturing the learning curves of realistic data sets with a teacher-student model</b>, Advances in Neural Information Processing Systems (Neurips) 2021. <br />[<a href="https://arxiv.org/abs/2102.08127">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Teacher-student models provide a framework in which the typical-case
performance of high-dimensional supervised learning can be described in
closed form. The assumptions of Gaussian i.i.d.&nbsp;input data underlying the canonical teacher-student model may, however, be perceived as too restrictive to capture the behaviour of realistic data sets. In this paper, we introduce a Gaussian covariate
generalisation of the model where the teacher and student can act on different spaces, generated with fixed, but generic feature maps. While still solvable in a closed form, this generalization is able to capture the learning curves for a broad range of realistic data sets, thus redeeming the potential of the teacher-student framework. Our contribution is then two-fold: First, we prove a rigorous formula for the asymptotic training loss and generalisation error. Second, we present a number of situations where the learning curve of the model captures the one of a emph{realisticdata set} learned with kernel regression and classification, with out-of-the-box feature maps such as random projections or scattering transforms, or with pre-learned ones - such as the features learned by training multi-layer neural networks. We discuss both the power and the limitations of the framework.
</div></p>
</li>
</ul>
<ul>
<li><p>Gerbelot, C., Abbara, A., & Krzakala, F. <b>Asymptotic Errors for Teacher-Student Convex Generalized Linear Models (or : How to Prove Kabashima's Replica Formula)</b>, 2020, IEEE Transactions on Information Theory. <br />[<a href="https://arxiv.org/abs/2006.06581">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> There has been a recent surge of interest in the study of asymptotic reconstruction performance in various cases of generalized linear estimation problems in the teacher-student setting, especially for the case of i.i.d standard normal matrices. Here, we go beyond these matrices, and prove an analytical formula for the reconstruction performance of convex generalized linear models with rotationally-invariant data matrices with arbitrary bounded spectrum, rigorously confirming a conjecture originally derived using the replica method from statistical physics. The formula includes many problems such as compressed sensing or sparse logistic classification. The proof is achieved by leveraging on message passing algorithms and the statistical properties of their iterates, allowing to characterize the asymptotic empirical distribution of the estimator. Our proof is crucially based on the construction of converging sequences of an oracle multi-layer vector approximate message passing algorithm, where the convergence analysis is done by checking the stability of an equivalent dynamical system. We illustrate our claim with numerical examples on mainstream learning methods such as sparse logistic regression and linear support vector classifiers, showing excellent agreement between moderate size simulation and the asymptotic prediction.
</div></p>
</li>
</ul>
<ul>
<li><p>Gerbelot, C., Abbara, A., & Krzakala, F. <b>Asymptotic errors for convex penalized linear regression beyond Gaussian matrices</b>, Conference On Learning Theory (COLT) 2020. PMLR, vol 125,1682-1713 <br />[<a href="[https://arxiv.org/abs/2002.04372">journal</a>, <a href="https://arxiv.org/abs/2002.04372">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> We consider the problem of learning a coefficient vector <img class="eq" src="eqs/2012421152428897345-130.png" alt="x_0 in {rm I!R}^{N}" style="vertical-align: -4px" /> from noisy linear observations <img class="eq" src="eqs/3209944599404123823-130.png" alt="y = Fx_{0}+w in {rm I!R}^{M}" style="vertical-align: -4px" /> in the high dimensional limit <img class="eq" src="eqs/7358547932384869682-130.png" alt="M,N to infty" style="vertical-align: -4px" /> with <img class="eq" src="eqs/2257599722597067843-130.png" alt="alpha equiv M/N" style="vertical-align: -5px" /> fixed. We provide a rigorous derivation of an explicit formula &#8201;&mdash;&#8201;first conjectured using heuristic methods from statistical physics&#8201;&mdash;&#8201; for the asymptotic mean squared error obtained by penalized convex regression estimators such as the LASSO or the elastic net, for a class of very generic random matrices corresponding to rotationally invariant data matrices with arbitrary spectrum. The proof is based on a convergence analysis of an oracle version of vector approximate message-passing (oracle-VAMP) and on the properties of its state evolution equations. Our method leverages on and highlights the link between vector approximate message-passing, Douglas-Rachford splitting and proximal descent algorithms, extending previous results obtained with i.i.d. matrices for a large class of problems. We illustrate our results on some concrete examples and show that even though they are asymptotic, our predictions agree remarkably well with numerics even for very moderate sizes.
</div></p>
</li>
</ul>
<ul>
<li><p>Ilton, M., Couchman, M. M., Gerbelot, C., Benzaquen, M., Fowler, P. D., Stone, H. A., &hellip; & Salez, T. <b>Capillary leveling of freestanding liquid nanofilms</b>, 2016, Physical review letters, 117(16), 167801. <br />[<a href="https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.117.167801">journal</a>, <a href="https://arxiv.org/abs/1602.05538">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> We report on the capillary-driven levelling of a topographical perturbation at the surface of a free-standing liquid nanofilm. The width of a stepped surface profile is found to evolve as the square root of time. The hydrodynamic model is in excellent agreement with the experimental data. In addition to exhibiting an analogy with diffusive processes, this novel system serves as a precise nanoprobe for the rheology of liquids at interfaces in a configuration that avoids substrate effects.
</div></p>
</li>
</ul>
<h2>Some talks, seminars and posters</h2>
 <ul>
<li><p>NYU CDS group seminar, November 2022 - <b>
</li>
<ul>
<li><p>NYU Courant Postdoc seminar, October 2022 - <b>
</li>
<ul>
<li><p>Les Houches Summer School on Statistical Physics and Machine Learning, July 2022 - <b>Graph-based approximate message passing iterations </b> <a href="poster_Houches2022.pdf">Poster</a></p>
</li>  
<li><p>INRIA/DYOGENE group seminar - <b>Statistical physics of learning : a mathematical perspective </b> <a href="Seminaire_INRIA.pdf">Slides</a></p>
</li>   
<li><p>Neurips@Paris 2021 - <b>Learning Gaussian Mixtures with Generalized Linear Models : Precise Asymptotics in High-dimension (15min Spotlight talk)</b></p>
</li>   
<li><p>Deepmath Conference, November 2021 - <b>Learning Gaussian Mixtures with Generalized Linear Models : Precise Asymptotics in High-dimension</b> <a href="poster_Deepmath.pdf">Poster</a></p>
</li>  
<li><p>CIRM workshop &ldquo;On Future Synergies for Stochastic and Learning Algorithms&rdquo;, September 2021 - <b>Graph-based Approximate Message Passing Iterations</b> <a href="poster_CIRM.pdf">Poster</a></p>
</li>
<li><p>Isaac Newton Institute for Mathematical Science, Theory of Deep Learning workshop, August 2021 - <b>Capturing the learning curves of realistic data sets with a teacher-student model</b> <a href="poster_newton.pdf">Poster</a></p>
</li>
<li><p>ICTP Youth in High Dimensions conference, July 2021 - <b>Beyond i.i.d. Gaussian models : exact asymptotics with realistic data</b> <a href="Gerbelot_ICTP_conf_2021.pdf">Slides</a>, <a href="https://www.youtube.com/watch?v=ZD-LwkQ57dQ,">Video</a></p>
</li>
<li><p>EPFL group seminar, June 2021  - <b>An AMP iteration for high-dimensional multiclass classification</b> <a href="Golosino_Multiclass.pdf">Slides</a></p>
</li>
<li><p>Les Houches Summer Workshop on Statistical Physics and Machine Learning, August 2020 - <b>How to prove Kabashima's replica formula</b> <a href="Les_Houches_talk_13min-1.pdf">Slides</a></p>
</li>
<li><p>ICTP seminar, April 2020,  <b>Rigorous results of statistical physics of simple machine learning models</b> <a href="Talk_Jean_GLM_orth_inv.pdf">Slides</a></p>
</li>
<li><p>Conference on Learning Theory, July 2020, <b>Asymptotic errors for convex penalized linear regression beyond Gaussian matrices</b> <a href="Talk_COLT_2020_15min.pdf">Slides</a>, <a href="https://www.youtube.com/watch?v=Y7BYES9PMsc,">Video</a></p>
</li>
<li><p>(Golosino seminar, Ecole Normale Superieure, November 2020  <b>Asymptotic errors for convex penalized linear regression beyond Gaussian matrices</b>)</p>
</li>
<li><p>Seminar at NTT Basic Research Labs, Japan, 2018 <b>Full Counting statistics of Electron Transport in a Biological Motor</b> <a href="SSDM2018_Abstract.pdf">Summary</a></p>
</li>
<li><p>Seminar at Gulliver Laboratory, ESPCI, Paris, 2016 <b>Capillary leveling of freestanding liquid nanofilms</b></p>
</li>
</ul>
<h2>Reviewing</h2>
<ul>
<li><p>Journal of Statistical Mechanics, Theory and Experiment (JSTAT) </div></p>
</li>
<li><p>IEEE Transactions on Information Theory </div></p>
</li>
<li><p>The Annals of Statistics </div></p>
</li>
<li><p>Information and Inference </div></p>
</li>  
<li><p>Advances in Neural Information Processing Systems (Neurips, 2021-) </div></p>
</li>
<li><p>International Conference on Machine Learning (ICML, 2022-) </div></p>
</li>
</ul>
<h2>Teaching</h2>
<ul>
<li><p> Graduate Computational Statistics - NYU Courant and Tandon School of Engineering - Fall 2022
</li>
</ul>  
<div id="footer">
<div id="footer-text">
Page generated 2021-09-30 19:18:15 CEST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128753599-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
